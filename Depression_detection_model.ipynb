{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Depression Detection in Social Media Text using Convolutional Neural Network</h1>\n",
    "<b> By Ezra Abah</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"C:/Users/Admin/Documents/Datasets/tweets_16million.csv\", encoding = \"ISO-8859-1\", names=[\"target\",\"id\",\"date\",\"flag\",\"user\",\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_usernames(text):\n",
    "    new_text=text.apply(lambda x: re.sub('@[\\w]+','', str(x)))\n",
    "    return new_text\n",
    "\n",
    "#def remove_hypertext(text):\n",
    "    \"\"\"Remove hypertext from text\"\"\"\n",
    "    new_text=text.str.replace('[^\\w\\s]','')\n",
    "    return new_text\n",
    "#def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    new_text=text.str.replace('[^\\w\\s]','')\n",
    "    return new_text\n",
    "\n",
    "def remove_html(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    new_text = text.apply(lambda x: re.sub(r'https?:\\/\\/(www\\.)?[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', str(x), flags=re.MULTILINE))\n",
    "    new_text = text.apply(lambda x: re.sub(r'[-a-zA-Z0–9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0–9@:%_\\+.~#?&//=]*)', '', str(x), flags=re.MULTILINE))\n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    nltk.download('stopwords')\n",
    "    stop = stopwords.words('english')\n",
    "    new_text= text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    return new_text\n",
    "\n",
    "def remove_digits(text):\n",
    "    \"\"\"Removes digit present in text\"\"\"\n",
    "    new_text=text.apply(lambda x: re.sub(r\"\\d\",'', str(x)))\n",
    "    return new_text\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"changes every text to lower case\"\"\"\n",
    "    new_text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    return new_text\n",
    "\n",
    "#def spelling_errors(text):\n",
    "    \"\"\"changes every text to lower case\"\"\"\n",
    "    new_text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    return new_text\n",
    "\n",
    "def lemmatize(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    new_text=text.apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "    return new_text\n",
    "#def stemmatize(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    new_text=text.str.replace('[^\\w\\s]','')\n",
    "    return new_text\n",
    "\n",
    "def clean (text):\n",
    "    new_text = remove_accented_chars(text)\n",
    "    new_text = remove_punctuation(text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of this data set is (1600000, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "      <td>800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    date    flag    user    text  hashtags\n",
       "target                                                  \n",
       "0       800000  800000  800000  800000  800000    800000\n",
       "4       800000  800000  800000  800000  800000    800000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Observe size of dataset\n",
    "shape = data.shape\n",
    "print(\"The size of this data set is \"+ str(shape))\n",
    "data.groupby('target').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view datatypes of individual columns\n",
    "\n",
    "types = data.dtypes\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for empty cells\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   target    1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   flag      1600000 non-null  object\n",
      " 4   user      1600000 non-null  object\n",
      " 5   text      1600000 non-null  object\n",
      " 6   hashtags  1600000 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 85.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove twitter handles\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "        \n",
    "    return input_txt\n",
    "\n",
    "# remove twitter handles (@user)\n",
    "data['text'] = np.vectorize(remove_pattern)(data['text'], \"@[\\w]*\")\n",
    "data['text'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove twitter handles (@user)\n",
    "data['text'] = np.vectorize(remove_pattern)(data['text'], \"http[\\w]*\")\n",
    "data['text'].head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Remove html codes\n",
    "def remove_html_tags(text):\n",
    "    return BeautifulSoup(text, 'html.parser').get_text()\n",
    "data['text'] = data['text'].apply(lambda x: remove_html_tags (x))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Punctuation\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s]','')\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize\n",
    "from textblob import Word\n",
    "data['text'] = data['text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmatize\n",
    "#def word_stemmer(text):\n",
    "#    stem_text = [PorterStemmer().stem(i) for i in text]\n",
    "#    return stem_text\n",
    "#data['text'] = data['text'].apply(lambda x: word_stemmer(x))\n",
    "#data.head()\n",
    "\n",
    "\n",
    "stem_text = PorterStemmer()\n",
    "data['text']=data['text'].apply(lambda x: \" \".join([stem_text.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = data['text'].apply(' '.join(data['text']).split()).value_counts()]\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=clean(data['text'])\n",
    "data['text'].head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_URL(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", text)\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        new_word = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_text.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(text):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        new_word = word.lower()\n",
    "        new_text.append(new_word)\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_text.append(new_word)\n",
    "    return new_text\n",
    "\n",
    "def replace_numbers(text):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_text = []\n",
    "    for word in text:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_text.append(new_word)\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def stem_words(text):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in text:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(text):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in text:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def clean (text):\n",
    "    text = remove_URL(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = replace_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text'].apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
